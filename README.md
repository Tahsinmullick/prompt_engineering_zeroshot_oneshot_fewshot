# 🎯 Understanding Prompt Engineering: Zero-Shot, One-Shot & Few-Shot Learning 🚀  

## 🧠 Overview  
This repository explores **Prompt Engineering**, a powerful technique that fine-tunes the way AI models interpret and generate responses.  

I focus on **Zero-Shot, One-Shot, and Few-Shot Learning** using the **FLAN-T5 base model** for **Dialogue Summarization**, analyzing how different approaches impact model behavior.  

📌 **Key Aspects of This Project:**  
- ✅ Comparative analysis of **Zero-Shot, One-Shot, and Few-Shot Learning** 🏆  
- ✅ Investigation of **In-Context Learning** 📖  
- ✅ Experimentation with **Token Size & Temperature Effects** 🔥  
- ✅ Utilization of **Hugging Face datasets** for real-world application 📡  

---

## 🛠️ What Is Prompt Engineering?  
**Prompt Engineering** is the art of crafting effective inputs for **AI models**, improving their ability to generate relevant, high-quality responses. The approach used can dramatically affect **accuracy, coherence, and creativity** in language models.  

### 🔹 Key Learning Modes:  
- **Zero-Shot Learning** – Model generates responses **without** prior examples. 🚀  
- **One-Shot Learning** – Model is provided **one** example to guide its understanding. 🎯  
- **Few-Shot Learning** – Model is fed **a few** examples to establish context. 🔍  

Each of these plays a critical role in refining **AI-generated outputs**, especially in applications like **text summarization, classification, and translation**.  

---

## 🔬 Experimental Setup  
### 🔹 **Using FLAN-T5 for Dialogue Summarization**  
We employ the **FLAN-T5 base model**, fine-tuning its ability to summarize dialogues effectively.  

### 🔹 **Understanding In-Context Learning**  
In-Context Learning allows models to adapt their responses by analyzing given prompts. Observations on its **effectiveness and limitations** are documented.  

### 🔹 **Exploring Token Size & Temperature**  
The model’s behavior is further analyzed by tweaking **Token Size** and **Temperature**, which control the **length** and **creativity** of outputs respectively.  

📡 **Dataset Source:** [Hugging Face](https://huggingface.co/datasets)  

---
